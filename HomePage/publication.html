<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications | MLO @ CityU</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap">
    <style>
        /* --- Global Reset & Typography --- */
        :root {
            --primary-color: #0056b3;
            --text-dark: #1a1a1a;
            --text-light: #555;
            --bg-color: #ffffff;
            --accent-bg: #f4f6f8;
            --border-color: #eaeaea;
            
            /* Button Colors */
            --color-pdf: #d32f2f;
            --color-code: #2e7d32;
            --color-slides: #e65100;
            --color-poster: #7b1fa2;
            --color-web: #0288d1;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.6;
            color: var(--text-dark);
            margin: 0;
            padding: 0;
            background-color: var(--bg-color);
        }

        a { text-decoration: none; color: inherit; transition: color 0.2s; }
        a:hover { color: var(--primary-color); }

        /* --- Layout --- */
        .container {
            width: 92%;             /* Adaptive width */
            max-width: 1200px;      /* Larger cap for wide screens */
            margin: auto;      /* Vertical spacing */
            padding: 20px;
        }

        /* --- Navigation --- */
        .nav {
            display: flex;
            gap: 25px;
            padding: 20px 0;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 40px;
            flex-wrap: wrap;
        }
        .nav a {
            font-weight: 600;
            font-size: 16px;
            color: var(--text-light);
        }
        .nav a:hover, .nav span.active {
            color: var(--primary-color);
        }

        /* --- Section Headers --- */
        h1 { font-size: 2.5rem; margin-bottom: 0.5rem; letter-spacing: -0.02em; }
        h2 { font-size: 1.75rem; margin-top: 3rem; margin-bottom: 1.5rem; border-bottom: 2px solid var(--accent-bg); padding-bottom: 10px; }
        h3 { font-size: 1rem; color: var(--text-light); font-weight: 400; margin-bottom: 2rem; }

        /* --- Publication Item Card --- */
        .pub-item {
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }
        
        .pub-title {
            font-size: 1.2rem;
            font-weight: 700;
            margin: 0 0 8px 0;
            color: #000;
        }

        .pub-authors {
            margin: 5px 0;
            color: var(--text-light);
            font-size: 1rem;
        }
        
        .pub-authors u {
            text-decoration: none;
            font-weight: 600;
            border-bottom: 2px solid #ccc;
        }

        .pub-venue {
            font-style: italic;
            color: var(--text-dark);
            font-weight: 500;
            margin-bottom: 10px;
        }

        /* --- Action Badges (PDF, Code, etc) --- */
        .pub-links {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 8px;
            align-items: center;
        }

        .badge {
            display: inline-flex;
            align-items: center;
            padding: 4px 12px;
            border-radius: 6px;
            font-size: 0.85rem;
            font-weight: 600;
            border: 1px solid #ddd;
            background-color: white;
            color: var(--text-light);
            cursor: pointer;
            transition: all 0.2s ease;
        }

        /* Abstract Toggle Button */
        .badge.abstract-toggle {
            background-color: var(--accent-bg);
            border-color: transparent;
            color: var(--text-dark);
        }
        .badge.abstract-toggle:hover {
            background-color: #e0e0e0;
        }

        /* PDF Button Style */
        .badge.pdf {
            color: var(--color-pdf);
            border-color: var(--color-pdf);
        }
        .badge.pdf:hover {
            background-color: var(--color-pdf);
            color: white;
        }

        /* Code Button Style */
        .badge.code {
            color: var(--color-code);
            border-color: var(--color-code);
        }
        .badge.code:hover {
            background-color: var(--color-code);
            color: white;
        }

        /* Slides Button Style */
        .badge.slides {
            color: var(--color-slides);
            border-color: var(--color-slides);
        }
        .badge.slides:hover {
            background-color: var(--color-slides);
            color: white;
        }

        /* Poster Button Style */
        .badge.poster {
            color: var(--color-poster);
            border-color: var(--color-poster);
        }
        .badge.poster:hover {
            background-color: var(--color-poster);
            color: white;
        }

        /* Web/Project Page Button Style */
        .badge.web {
            color: var(--color-web);
            border-color: var(--color-web);
        }
        .badge.web:hover {
            background-color: var(--color-web);
            color: white;
        }


        /* --- Abstract Section --- */
        .abstract-content {
            display: none;
            margin-top: 15px;
            padding: 15px;
            background-color: var(--accent-bg);
            border-radius: 8px;
            font-size: 0.95rem;
            color: #444;
            line-height: 1.7;
            border-left: 4px solid var(--primary-color);
        }
        
        .abstract-content.show {
            display: block;
            animation: fadeIn 0.3s ease-in-out;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-5px); }
            to { opacity: 1; transform: translateY(0); }
        }

        @media (max-width: 600px) {
            h1 { font-size: 2rem; }
            .nav { justify-content: center; gap: 15px; }
        }
    </style>
    <script>
        function toggleAbstract(id, btn) {
            const content = document.getElementById(id);
            const isHidden = !content.classList.contains('show');
            
            if (isHidden) {
                content.classList.add('show');
                btn.innerHTML = "Hide Abstract";
            } else {
                content.classList.remove('show');
                btn.innerHTML = "Abstract";
            }
        }
    </script>
</head>
<body>

    <div class="container">
        <div class="nav">
            <a href="./index.html">Home</a>
            <a href="./group.html">Group</a>
            <a href="./teaching.html">Teaching</a>
            <a href="./talk.html">Talks</a>
            <span class="active" style="font-weight: 700; color: var(--primary-color);">Publications</span>
            <a href="./openings.html">Open Positions</a>
        </div>

        <h1>Publications</h1>
        <h3>In reverse chronological order. <sup>*</sup> indicates equal contribution.</h3>

        <h2>Preprints</h2>

        <div class="pub-item">
            <h4 class="pub-title">EFS: Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models</h4>
            <div class="pub-authors">Haochen Luo, Yuan Zhang, <u>Chen Liu</u>.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-EFS', this)">Abstract</button>
                <a href="doc/EFS/EFS_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
            </div>
            <div id="abs-EFS" class="abstract-content">
                Sparse portfolio optimization is a fundamental yet challenging problem in quantitative finance, since traditional approaches heavily relying on historical return statistics and static objectives can hardly adapt to dynamic market regimes. To address this issue, we propose Evolutionary Factor Search (EFS), a novel framework that leverages large language models (LLMs) to automate the generation and evolution of alpha factors for sparse portfolio construction. By reformulating the asset selection problem as a top-m ranking task guided by LLM-generated factors, EFS incorporates an evolutionary feedback loop to iteratively refine the factor pool based on performance. Extensive experiments on five Fama-French benchmark datasets and three real-market datasets (US50, HSI45 and CSI300) demonstrate that EFS significantly outperforms both statistical-based and optimization-based baselines, especially in larger asset universes and volatile conditions. Comprehensive ablation studies validate the importance of prompt composition, factor diversity, and LLM backend choice. Our results highlight the promise of language-guided evolution as a robust and interpretable paradigm for portfolio optimization under structural constraints.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Hidden State Differential Private Mini-Batch Block Coordinate Descent for Multi-convexity Optimization</h4>
            <div class="pub-authors">Ding Chen, <u>Chen Liu</u>.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-DPHSA', this)">Abstract</button>
                <a href="doc/DPHSA/DPHSA_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
            </div>
            <div id="abs-DPHSA" class="abstract-content">
                We investigate the differential privacy (DP) guarantees under the hidden state assumption (HSA) for multi-convex problems. Recent analyses of privacy loss under the hidden state assumption have relied on strong assumptions such as convexity, thereby limiting their applicability to practical problems. In this paper, we introduce the Differential Privacy Mini-Batch Block Coordinate Descent (DP-MBCD) algorithm, accompanied by the privacy loss accounting methods under the hidden state assumption. Our proposed methods apply to a broad range of classical non-convex problems which are or can be converted to multi-convex problems, such as matrix factorization and neural network training. In addition to a tighter bound for privacy loss, our theoretical analysis is also compatible with proximal gradient descent and adaptive calibrated noise scenarios.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage</h4>
            <div class="pub-authors">Xinping Chen, <u>Chen Liu</u>.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-GIT', this)">Abstract</button>
                <a href="doc/GIT/GIT_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
            </div>
            <div id="abs-GIT" class="abstract-content">
                We propose Gradient Inversion Transcript (GIT), a novel generative approach for reconstructing training data from leaked gradients. GIT employs a generative attack model, whose architecture is tailored to align with the structure of the leaked model based on theoretical analysis. Once trained offline, GIT can be deployed efficiently and only relies on the leaked gradients to reconstruct the input data, rendering it applicable under various distributed learning environments. When used as a prior for other iterative optimization-based methods, GIT not only accelerates convergence but also enhances the overall reconstruction quality. GIT consistently outperforms existing methods across multiple datasets and demonstrates strong robustness under challenging conditions, including inaccurate gradients, data distribution shifts and discrepancies in model parameters.
            </div>
        </div>

        <h2>Refereed Papers</h2>

        <div class="pub-item">
            <h4 class="pub-title">AlphaBench: Benchmarking Large Language Models in Formulaic Alpha Factor Mining</h4>
            <div class="pub-authors">Haochen Luo, Ho Tin Ko, Jiandong Chen, David Sun, Yuan Zhang, <u>Chen Liu</u>.</div>
            <div class="pub-venue">International Conference on Learning Representations (ICLR) 2026.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-AlphaBench', this)">Abstract</button>
                <a href="./doc/AlphaBench/AlphaBench_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="https://alphabench.cc" target="_blank" class="badge web">Project Page</a>
            </div>
            <div id="abs-AlphaBench" class="abstract-content">
                Formulaic alpha factor mining (FAFM) is a central problem in quantitative investment, where interpretable formulas are designed to extract predictive signals from historical financial series. With the emergence of large language models (LLMs), recent studies have begun to explore their roles in FAFM, yet their capabilities across different tasks and configurations remain unclear. In this work, we introduce AlphaBench, the first systematic benchmark for evaluating LLMs in FAFM. AlphaBench covers three core tasks, including factor generation, factor evaluation, and factor searching, which are all popular tasks integrated in the workflow of quantitative researchers. Beyond task-level evaluation, we further analyze how different LLM settings, including model type, prompting paradigm, and reasoning strategy, influence performance. Our experiments on a range of open-source and closed-source models reveal that LLMs hold strong potential in automating factor mining, while also facing persistent challenges in robustness, search efficiency, and practical usability.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Sparse-PGD: A Unified Framework for Sparse Adversarial Perturbations Generation</h4>
            <div class="pub-authors">Xuyang Zhong, <u>Chen Liu</u>.</div>
            <div class="pub-venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2025.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-SPGD', this)">Abstract</button>
                <a href="./doc/SPGD/SPGD_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="https://github.com/CityU-MLO/sPGD" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-SPGD" class="abstract-content">
                This work studies sparse adversarial perturbations, including both unstructured and structured ones. We propose a framework based on a white-box PGD-like attack method named Sparse-PGD to effectively and efficiently generate such perturbations. Furthermore, we combine Sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against unstructured and structured sparse adversarial perturbations. Moreover, the efficiency of Sparse-PGD enables us to conduct adversarial training to build robust models against various sparse perturbations. Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios. More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers</h4>
            <div class="pub-authors">Xuyang Zhong, Haochen Luo, <u>Chen Liu</u>.</div>
            <div class="pub-venue">Neural Information Processing Systems (NeurIPS) 2025.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-DualOptim', this)">Abstract</button>
                <a href="./doc/DualOptim/DualOptim_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="./doc/DualOptim/DualOptim_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="./doc/DualOptim/DualOptim_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/CityU-MLO/DualOptim" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-DualOptim" class="abstract-content">
                Existing machine unlearning (MU) approaches exhibit significant sensitivity to hyperparameters, requiring meticulous tuning that limits practical deployment. In this work, we first empirically demonstrate the instability and suboptimal performance of existing popular MU methods when deployed in different scenarios. To address this issue, we propose Dual Optimizer (DualOptim), which incorporates adaptive learning rate and decoupled momentum factors. Empirical and theoretical evidence demonstrates that DualOptim contributes to effective and stable unlearning. Through extensive experiments, we show that DualOptim can significantly boost MU efficacy and stability across diverse tasks, including image classification, image generation, and large language models, making it a versatile approach to empower existing MU algorithms.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Data Selection Matters: Towards Robust Instruction Tuning of Large Multimodal Models</h4>
            <div class="pub-authors">Xu Yang, <u>Chen Liu</u>, Ying Wei.</div>
            <div class="pub-venue">Neural Information Processing Systems (NeurIPS) 2025.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-DataSelectionVLM', this)">Abstract</button>
                <a href="./doc/DataSelectionVLM/DataSelectionVLM_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="./doc/DataSelectionVLM/DataSelectionVLM_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="./doc/DataSelectionVLM/DataSelectionVLM_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/xyang583/ARDS" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-DataSelectionVLM" class="abstract-content">
                Selecting a compact set of visual instruction–following data has emerged as an effective way to align large multimodal models with human intentions while avoiding the cost of full-dataset training. Yet we observe that both full-data and state-of-the-art-method selected-data instruction tuning can inherit underlying dataset biases, leading to biased model behavior due to position bias and spurious correlations. We introduce ARDS, a robustness-aware targeted visual instruction-selection framework that explicitly targets these weaknesses, sidestepping the need for access to downstream data or time-consuming gradient computation. Specifically, we first generate the worst-case evaluation subgroups through visual and textual task-specific perturbations. The robust training mixture is then constructed by prioritizing the samples that are semantically closer to these subgroups in the rich multimodal embedding space. Extensive experiments show that ARDS substantially boosts both robustness and data efficiency for visual instruction tuning. We also showcase that the robust mixtures produced with a smaller model transfer effectively to larger architectures.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Understanding and Improving Fast Adversarial Training against $l_0$ Bounded Perturbations</h4>
            <div class="pub-authors">Xuyang Zhong, Yixiao Huang, <u>Chen Liu</u>.</div>
            <div class="pub-venue">Neural Information Processing Systems (NeurIPS) 2025.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-FastSPGD', this)">Abstract</button>
                <a href="./doc/FastSPGD/FastSPGD_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="./doc/FastSPGD/FastSPGD_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="./doc/FastSPGD/FastSPGD_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/CityU-MLO/sPGD" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-FastSPGD" class="abstract-content">
                This paper studies fast adversarial training against sparse adversarial perturbations bounded by l0 norm. We demonstrate the challenges of employing 1-step attacks on l0 bounded perturbations for fast adversarial training, including degraded performance and the occurrence of catastrophic overfitting (CO). We highlight that CO in l0 adversarial training is caused by sub-optimal perturbation locations of 1-step attack. Theoretical and empirical analyses reveal that the loss landscape of l0 adversarial training is more craggy compared to its l-infinity, l2 and l1 counterparts. Moreover, we corroborate that the craggy loss landscape can aggravate CO. To address these issues, we propose Fast-LS-l0 that incorporates soft labels and the trade-off loss function to smooth the adversarial loss landscape. Extensive experiments demonstrate our method can overcome the challenge of catastrophic overfitting, achieve state-of-the-art performance, and narrow down the performance gap between 1-step and multi-step adversarial training against sparse attacks.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Towards Mitigating Architecture Overfitting in Dataset Distillation</h4>
            <div class="pub-authors">Xuyang Zhong, <u>Chen Liu</u>.</div>
            <div class="pub-venue">IEEE Transactions on Neural Networks and Learning Systems 2025.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-DataDistillationArchOverfit', this)">Abstract</button>
                <a href="doc/DataDistillationArchOverfit/DataDistillationArchOverfit_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="https://github.com/CityU-MLO/mitigate_architecture_overfitting" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-DataDistillationArchOverfit" class="abstract-content">
                Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">On the Impact of Hard Adversarial Instances on Overfitting in Adversarial Training</h4>
            <div class="pub-authors"><u>Chen Liu</u>, Zhichao Huang, Mathieu Salzmann, Tong Zhang, Sabine Süsstrunk.</div>
            <div class="pub-venue">Journal of Machine Learning Research (JMLR) 2024</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-HardAdversarialInstances', this)">Abstract</button>
                <a href="doc/HardAdversarialInstances/HardAdversarialInstances_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/HardAdversarialInstances/HardAdversarialInstances_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="doc/HardAdversarialInstances/HardAdversarialInstances_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/IVRL/RobustOverfit-HardInstance" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-HardAdversarialInstances" class="abstract-content">
                Adversarial training is a popular method to robustify models against adversarial attacks. However, it exhibits much more severe overfitting than training on clean inputs. In this work, we investigate this phenomenon from the perspective of training instances, i.e., training input-target pairs. Based on a quantitative metric measuring instances’ difficulty, we analyze the model's behavior on training instances of different difficulty levels. This lets us show that the decay in generalization performance of adversarial training is a result of the model's attempt to fit hard adversarial instances. We theoretically verify our observations for both linear and general nonlinear models, proving that models trained on hard instances have worse generalization performance than ones trained on easy instances. Furthermore, we prove that the difference in the generalization gap between models trained by instances of different difficulty levels increases with the size of the adversarial budget. Finally, we conduct case studies on methods mitigating adversarial overfitting in several scenarios. Our analysis shows that methods successfully mitigating adversarial overfitting all avoid fitting hard adversarial instances, while ones fitting hard adversarial instances do not achieve true robustness.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Mixture of Adversarial LoRAs: Boosting Robust Generalization in Meta-tuning</h4>
            <div class="pub-authors">Xu Yang, <u>Chen Liu</u>, Ying Wei.</div>
            <div class="pub-venue">Neural Information Processing Systems (NeurIPS) 2024.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-MixedLoRA', this)">Abstract</button>
                <a href="doc/MixedLoRA/MixedLoRA_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/MixedLoRA/MixedLoRA_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="doc/MixedLoRA/MixedLoRA_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/xyang583/AMT" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-MixedLoRA" class="abstract-content">
                This paper introduces AMT, an adversarial meta-tuning methodology, to boost the robust generalization of pre-trained models in the out-of-domain (OOD) few-shot learning. To address the challenge of transferring knowledge from source domains to unseen target domains, we construct the robust LoRAPool by meta-tuning LoRAs with double perturbations on both inputs and singular values and vectors at varying robustness levels. On top of that, we introduce a simple yet effective test-time merging mechanism for adaptively merging discriminative LoRAs for test-time task customization. Extensive evaluations demonstrate that the AMT brings substantial improvements over previous state-of-the-art methods across a range of OOD few-shot image classification tasks on three benchmarks, confirming the effectiveness of our approach to boost the robust generalization of pre-trained models.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Controlling the Fidelity and Diversity of Deep Generative Models via Pseudo Density</h4>
            <div class="pub-authors">Shuangqi Li, <u>Chen Liu</u>, Tong Zhang, Hieu Le, Sabine Süsstrunk, Mathieu Salzmann.</div>
            <div class="pub-venue">Transactions on Machine Learning Research (TMLR) 2024.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-BiasDGM', this)">Abstract</button>
                <a href="doc/BiasDGM/BiasDGM_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/BiasDGM/BiasDGM_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="https://github.com/doub7e/pseudo-diversity" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-BiasDGM" class="abstract-content">
                We introduce an approach to bias deep generative models, such as GANs and diffusion models, towards generating data with either enhanced fidelity or increased diversity. Our approach involves manipulating the distribution of training and generated data through a novel metric for individual samples, named pseudo density, which is based on the nearest-neighbor information from real samples. Our approach offers three distinct techniques to adjust the fidelity and diversity of deep generative models: 1) Per-sample perturbation, enabling precise adjustments for individual samples towards either more common or more unique characteristics; 2) Importance sampling during model inference to enhance either fidelity or diversity in the generated data; 3) Fine-tuning with importance sampling, which guides the generative model to learn an adjusted distribution, thus controlling fidelity and diversity. Furthermore, our fine-tuning method demonstrates the ability to improve the Frechet Inception Distance (FID) for pre-trained generative models with minimal iterations.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations</h4>
            <div class="pub-authors">Xuyang Zhong, Yixiao Huang, <u>Chen Liu</u></div>
            <div class="pub-venue">International Conference on Machine Learning (ICML) 2024.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-SparsePGD', this)">Abstract</button>
                <a href="doc/SparsePGD/SparsePGD_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/SparsePGD/SparsePGD_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="doc/SparsePGD/SparsePGD_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/CityU-MLO/sPGD" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-SparsePGD" class="abstract-content">
                This work studies sparse adversarial perturbations bounded by l0 norm. We propose a white-box PGD-like attack method named sparse-PGD to effectively and efficiently generate such perturbations. Furthermore, we combine sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against l0 bounded adversarial perturbations. Moreover, the efficiency of sparse-PGD enables us to conduct adversarial training to build robust models against sparse perturbations. Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios. More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Fast Adversarial Training with Adaptive Step Size</h4>
            <div class="pub-authors">Zhichao Huang, Yanbo Fan, <u>Chen Liu</u>, Weizhong Zhang, Yong Zhang, Mathieu Salzmann, Sabine Süsstrunk, Jue Wang</div>
            <div class="pub-venue">IEEE Transactions on Image Processing 2023.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-FastAdaptiveStepSize', this)">Abstract</button>
                <a href="doc/FastAdaptiveStepSize/FastAdaptiveStepSize.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="https://github.com/HuangZhiChao95/ATAS" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-FastAdaptiveStepSize" class="abstract-content">
                While adversarial training and its variants have shown to be the most effective algorithms to defend against adversarial attacks, their extremely slow training process makes it hard to scale to large datasets like ImageNet. The key idea of recent works to accelerate adversarial training is to substitute multi-step attacks (e.g., PGD) with single-step attacks (e.g., FGSM). However, these single-step methods suffer from catastrophic overfitting, where the accuracy against PGD attack suddenly drops to nearly 0% during training, and the network totally loses its robustness. In this work, we study the phenomenon from the perspective of training instances. We show that catastrophic overfitting is instance-dependent, and fitting instances with larger input gradient norm is more likely to cause catastrophic overfitting. Based on our findings, we propose a simple but effective method, Adversarial Training with Adaptive Step size (ATAS). ATAS learns an instance-wise adaptive step size that is inversely proportional to its gradient norm. Our theoretical analysis shows that ATAS converges faster than the commonly adopted non- adaptive counterparts. Empirically, ATAS consistently mitigates catastrophic overfitting and achieves higher robust accuracy on CIFAR10, CIFAR100, and ImageNet when evaluated on various adversarial budgets.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Towards Stable and Efficient Adversarial Training against $l_1$ Bounded Adversarial Attacks</h4>
            <div class="pub-authors">Yulun Jiang<sup>*</sup>, <u>Chen Liu</u><sup>*</sup>, Zhichao Huang, Mathieu Salzmann, Sabine Süsstrunk</div>
            <div class="pub-venue">International Conference on Machine Learning (ICML) 2023</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-FastAdvL1', this)">Abstract</button>
                <a href="doc/FastAdvL1/FastAdvL1_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/FastAdvL1/FastAdvL1_Poster" target="_blank" class="badge poster">Poster</a>
                <a href="doc/FastAdvL1/FastAdvL1_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/IVRL/FastAdvL1" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-FastAdvL1" class="abstract-content">
                We address the problem of stably and efficiently training a deep neural network robust to adversarial perturbations bounded by an l1 norm. We demonstrate that achieving robustness against l1-bounded perturbations is more challenging than in the l2 or l∞ cases, because adversarial training against l1-bounded perturbations is more likely to suffer from catastrophic overfitting and yield training instabilities. Our analysis links these issues to the coordinate descent strategy used in existing methods. We address this by introducing Fast-EG-l1, an efficient adversarial training algorithm based on Euclidean geometry and free of coordinate descent. Fast-EG-l1 comes with no additional memory costs and no extra hyper-parameters to tune. Our experimental results on various datasets demonstrate that Fast-EG-l1 yields the best and most stable robustness against l1-bounded adversarial attacks among the methods of comparable computational complexity.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Robust Binary Models by Pruning Randomly-initialized Networks</h4>
            <div class="pub-authors"><u>Chen Liu</u><sup>*</sup>, Ziqi Zhao<sup>*</sup>, Sabine Süsstrunk, Mathieu Salzmann.</div>
            <div class="pub-venue">Advances in Neural Information Processing Systems (NeurIPS) 2022</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-RandomPrune', this)">Abstract</button>
                <a href="doc/RandomPrune/RandomPrune_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/RandomPrune/RandomPrune_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="doc/RandomPrune/RandomPrune_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/IVRL/RobustBinarySubNet" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-RandomPrune" class="abstract-content">
                Robustness to adversarial attacks was shown to require a larger model capacity, and thus a larger memory footprint. In this paper, we introduce an approach to obtain robust yet compact models by pruning randomly-initialized binary networks. Unlike adversarial training, which learns the model parameters, we initialize the model parameters as either +1 or −1, keep them fixed, and find a subnetwork structure that is robust to attacks. Our method confirms the Strong Lottery Ticket Hypothesis in the presence of adversarial attacks, and extends this to binary networks. Furthermore, it yields more compact networks with competitive performance than existing works by 1) adaptively pruning different network layers; 2) exploiting an effective binary initialization scheme; 3) incorporating a last batch normalization layer to improve training stability. Our experiments demonstrate that our approach not only always outperforms the state-of-the-art robust binary networks, but also can achieve accuracy better than full-precision ones on some datasets. Finally, we show the structured patterns of our pruned binary networks.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Training Provably Robust Models by Polyhedral Envelope Regularization</h4>
            <div class="pub-authors"><u>Chen Liu</u>, Mathieu Salzmann, Sabine Süsstrunk.</div>
            <div class="pub-venue">IEEE Transactions on Neural Networks and Learning Systems 2021.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-PolyhedralEnvelope', this)">Abstract</button>
                <a href="doc/PolyhedralEnvelope/PolyhedralEnvelope_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/PolyhedralEnvelope/PolyhedralEnvelope_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/liuchen11/PolyEnvelope" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-PolyhedralEnvelope" class="abstract-content">
                Training certifiable neural networks enables us to obtain models with robustness guarantees against adversarial attacks. In this work, we introduce a framework to obtain a provable adversarial-free region in the neighborhood of the input data by a polyhedral envelope, which yields more fine-grained certified robustness than existing methods. We further introduce polyhedral envelope regularization (PER) to encourage larger adversarial-free regions and thus improve the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks of different architectures and with general activation functions. Compared with state of the art, PER has negligible computational overhead; it achieves better robustness guarantees and accuracy on the clean data in various settings
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">On the Loss Landscape of Adversarial Training: Identifying Challenges and How to Overcome Them</h4>
            <div class="pub-authors"><u>Chen Liu</u>, Mathieu Salzmann, Tao Lin, Ryota Tomioka, Sabine Süsstrunk.</div>
            <div class="pub-venue">Advances in Neural Information Processing Systems (NeurIPS) 2020.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-AdversaryLossLandscape', this)">Abstract</button>
                <a href="doc/AdversaryLossLandscape/AdversaryLossLandscape_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/AdversaryLossLandscape/AdversaryLossLandscape_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="doc/AdversaryLossLandscape/AdversaryLossLandscape_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/liuchen11/AdversaryLossLandscape" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-AdversaryLossLandscape" class="abstract-content">
                We analyze the influence of adversarial training on the loss landscape of machine learning models. To this end, we first provide analytical studies of the properties of adversarial loss functions under different adversarial budgets. We then demonstrate that the adversarial loss landscape is less favorable to optimization, due to increased curvature and more scattered gradients. Our conclusions are validated by numerical analyses, which show that training under large adversarial budgets impede the escape from suboptimal random initialization, cause non-vanishing gradients and make the model find sharper minima. Based on these observations, we show that a periodic adversarial scheduling (PAS) strategy can effectively overcome these challenges, yielding better results than vanilla adversarial training while being much less sensitive to the choice of learning rate.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">On Certifying Non-uniform Bounds against Adversarial Attacks</h4>
            <div class="pub-authors"><u>Chen Liu</u>, Ryota Tomioka, Volkan Cevher.</div>
            <div class="pub-venue">International Conference on Machine Learning (ICML) 2019.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-CertifyNonUniformBounds', this)">Abstract</button>
                <a href="doc/CertifyNonUniformBounds/CertifyNonUniformBounds_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/CertifyNonUniformBounds/CertifyNonUniformBounds_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="doc/CertifyNonUniformBounds/CertifyNonUniformBounds_Slides.pdf" target="_blank" class="badge slides">Slides</a>
                <a href="https://github.com/liuchen11/Certify_Nonuniform_Bounds" target="_blank" class="badge code">Code</a>
            </div>
            <div id="abs-CertifyNonUniformBounds" class="abstract-content">
                This work studies the robustness certification problem of neural network models, which aims to find certified adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models. We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method. Our experiments show the non-uniform bounds have larger volumes than uniform ones and the geometric similarity of the non-uniform bounds gives a quantitative, data- agnostic metric of input features' robustness. Further, compared with normal models, the robust models have even larger non-uniform bounds and better interpretability.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Finding Mixed Nash Equilibria of Generative Adversarial Networks</h4>
            <div class="pub-authors">Ya-Ping Hsieh, <u>Chen Liu</u>, Volkan Cevher.</div>
            <div class="pub-venue">International Conference on Machine Learning (ICML) 2019.</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-MirrorGAN', this)">Abstract</button>
                <a href="doc/MirrorGAN/MirrorGAN_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="doc/MirrorGAN/MirrorGAN_Poster.pdf" target="_blank" class="badge poster">Poster</a>
                <a href="doc/MirrorGAN/MirrorGAN_Slides.pdf" target="_blank" class="badge slides">Slides</a>
            </div>
            <div id="abs-MirrorGAN" class="abstract-content">
                Generative adversarial networks (GANs) are known to achieve the state-of-the-art performance on various generative tasks, but these results come at the expense of a notoriously difficult training phase. Current training strategies typically draw a connection to optimization theory, whose scope is restricted to local convergence due to the presence of non-convexity. In this work, we tackle the training of GANs by rethinking the problem formulation from the mixed Nash Equilibria (NE) perspective. Via a classical lifting trick, we show that essentially all existing GAN objectives can be relaxed into their mixed strategy forms, whose global optima can be solved via sampling, in contrast to the exclusive use of optimization framework in previous work. We further propose a mean-approximation sampling scheme, which allows to systematically exploit methods for bi-affine games to delineate novel, practical training algorithms of GANs. Finally, we provide experimental evidence that our approach yields comparable or superior results to contemporary training algorithms, and outperforms classical methods such as SGD, Adam, and RMSProp.
            </div>
        </div>

        <div class="pub-item">
            <h4 class="pub-title">Consistent 3D Rendering in Medical Imaging</h4>
            <div class="pub-authors"><u>Chen Liu</u>, Shun Miao, Kaloian Petkov, Sandra Sudarsky, Daphne Yu, Tommaso Mansi.</div>
            <div class="pub-venue">European Patent No. 18160956.1</div>
            <div class="pub-links">
                <button class="badge abstract-toggle" onclick="toggleAbstract('abs-Consistent3DRender', this)">Abstract</button>
                <a href="doc/Consistent3DRender/Consistent3DRender_PDF.pdf" target="_blank" class="badge pdf">PDF</a>
                <a href="https://data.epo.org/publication-server/rest/v1.0/publication-dates/20180912/patents/EP3373245NWA2/document.pdf" target="_blank" class="badge web">Metadata@EPO</a>
            </div>
            <div id="abs-Consistent3DRender" class="abstract-content">
                For three-dimensional rendering, a machine-learnt model is trained to generate representation vectors for rendered images formed with different rendering parameter settings. The distances between representation vectors of the images to a reference are used to select the rendered image and corresponding rendering parameters that provides a consistency with the reference. In an additional or different embodiment, optimized pseudorandom sequences are used for physically-based rendering. The random number generator seed is selected to improve the convergence speed of the renderer and to provide higher quality images, such as providing images more rapidly for training compared to using non-optimized seed selection.
            </div>
        </div>

    </div>
</body>
</html>